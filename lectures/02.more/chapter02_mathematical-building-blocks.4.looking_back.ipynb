{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eb21a0-3561-489b-aba7-db72d7dacf16",
   "metadata": {},
   "source": [
    "## Looking back at our first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50699b2-7590-4aa4-8fc1-c5bb45ccc7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "230ca182",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1186b1-721c-47d2-8081-157d63c8d4f1",
   "metadata": {},
   "source": [
    "### A simple Dense class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d9590-8044-4560-a5c9-c62c16db3575",
   "metadata": {},
   "source": [
    "```python\n",
    "output = activation(matmul(input, W) + b)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3af1476-0085-4391-95ea-7fb98ed24ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activation=None):\n",
    "        self.activation = activation\n",
    "        self.W = keras.Variable(\n",
    "            # Creates a matrix W of shape (input_size, output_size),\n",
    "            # initialized with random values drawn from a uniform\n",
    "            # distribution\n",
    "            shape=(input_size, output_size), initializer=\"uniform\"\n",
    "        )\n",
    "        # Creates a vector b of shape (output_size,), initialized with zeros\n",
    "        self.b = keras.Variable(shape=(output_size,), initializer=\"zeros\")\n",
    "\n",
    "    # Applies the forward pass\n",
    "    def __call__(self, inputs):\n",
    "        # Store input for backpropagation\n",
    "        self._stored_input = inputs\n",
    "        x = keras.ops.matmul(inputs, self.W)\n",
    "        x = x + self.b\n",
    "        # Store pre-activation value for backpropagation\n",
    "        self._stored_pre_activation = x\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    # The convenience method for retrieving the layer's weights\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28d2e0-839d-42d2-bce9-fe32bbd5d015",
   "metadata": {},
   "source": [
    "### A simple Sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bae517c-0f03-423e-9cc3-a273cd1a6693",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "        return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b414dc44-9cdf-40be-b898-f39a727c4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveSequential(\n",
    "    [\n",
    "        NaiveDense(input_size=28 * 28, output_size=512, activation=keras.ops.relu),\n",
    "        NaiveDense(input_size=512, output_size=10, activation=keras.ops.softmax),\n",
    "    ]\n",
    ")\n",
    "assert len(model.weights) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1e0f3-a71b-412f-a200-c19453f70ba1",
   "metadata": {},
   "source": [
    "### A batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b283d5-39ff-4227-992a-318ac04286b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    "\n",
    "    def next(self):\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b9922",
   "metadata": {},
   "source": [
    "### Manual gradient computation (without GradientTape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca8e5b-3bc3-4fba-bc7b-2dd20f1a7de5",
   "metadata": {},
   "source": [
    "#### Gradient derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe2921-68d3-4f93-ab64-868aa7b469e2",
   "metadata": {},
   "source": [
    "For the last layer with softmax activation and sparse categorical cross-entropy loss, we can derive a simple formula for the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1ae3b-9b8d-4339-9593-7e10962ac60a",
   "metadata": {},
   "source": [
    "##### The loss function: Cross-entropy & Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c5d83-0a8b-4448-8025-580e0d0a4953",
   "metadata": {},
   "source": [
    "For a single sample with true label $y$ (an integer) and predicted probabilities $\\mathbf{p} = [p_1, p_2, ..., p_C]$ (where $C$ is the number of classes), the cross-entropy loss is:\n",
    "\n",
    "$$L = -\\log(p_y)$$\n",
    "\n",
    "We want to compute $\\frac{\\partial L}{\\partial p_i}$ for all $i$.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial p_y} = \\frac{ \\partial (-\\log(p_y)) } { \\partial p_y} = {\\color{blue} -\\frac{1}{p_y} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0e3be-aefc-4e4a-a04a-45f33a2f326a",
   "metadata": {},
   "source": [
    "##### The softmax function & Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e3173-e687-4fd2-b0d6-23fb4942d212",
   "metadata": {},
   "source": [
    "Given pre-softmax logits $\\mathbf{z} = [z_1, z_2, ..., z_C]$, the softmax probabilities are:\n",
    "\n",
    "$$p_i = \\frac{e^{z_i} }{\\sum_{j=1}^{C} e^{z_j} }$$\n",
    "\n",
    "The derivative of softmax is:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial p_i}{\\partial z_y} & = \\frac{\\partial  \\frac{ e^{z_i} \\quad (= g(x) ) }{\\sum_{j=1}^{C} e^{z_j}  \\quad ( = h(x) )} }{\\partial z_y} & | \\quad \\small \\left( \\frac{ g(x) }{ h(x) } \\right)' = \\frac{ {\\color{pink} g'(x) } {\\color{cyan} h(x) }- { \\color{purple} h'(x) } { \\color{orange} g(x) } }{ {\\color{magenta} h(x)^2 } } \\\\\n",
    "\\\\\n",
    "& =\n",
    "\\begin{cases}\n",
    "  \\begin{aligned}\n",
    "    & = \\frac{ {\\color{pink} e^{z_y} } {\\color{cyan}  \\sum_{j=1}^{C} e^{z_j} } -  { \\color{purple} e^{z_y} }  { \\color{orange}  e^{z_i} } }{  { \\color{magenta} { \\sum_{j=1}^{C} e^{z_j} }^2 } } \\\\\n",
    "    & = \\frac{ e^{z_y} }{  \\sum_{j=1}^{C} e^{z_j} } \\frac{ ( \\sum_{j=1}^{C} e^{z_j} -  e^{z_i} ) }{ \\sum_{j=1}^{C} e^{z_j} } \\\\\n",
    "    \\\\\n",
    "    & = \\underline{ { \\color{green} p_y(1 - p_i) }  }\n",
    "  \\end{aligned}\n",
    "  & \\text{if } i = y,\\\\\n",
    "  \\\\\n",
    "  \\\\\n",
    "  \\begin{aligned}\n",
    "  & = \\frac{ \\overbrace{ 0 }^{ { \\color{pink} 0 } \\cdot {\\color{cyan}  \\sum_{j=1}^{C} e^{z_j} } } - { \\color{purple} e^{z_y} }   { \\color{orange} e^{z_i} } }{  { \\color{magenta} { \\sum_{j=1}^{C} e^{z_j} }^2 } } \\\\\n",
    "  & = \\frac{-  e^{z_y} }{  \\sum_{j=1}^{C} e^{z_j} }  \\frac{ e^{z_i} }{ \\sum_{j=1}^{C} e^{z_j} } \\\\  \n",
    "  \\\\\n",
    "  & = \\underline{ { \\color{green} -p_y \\cdot p_i } } \n",
    "  & \\text{otherwise.}\n",
    "  \\end{aligned}\n",
    "\\end{cases} \n",
    "\\end{align}$$\n",
    "\n",
    "See [this page](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/).\n",
    "\n",
    "**Case 1**: $i = y$ (the true class):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial z_y} & = { \\color{blue} -\\frac{1}{p_y} } \\cdot \\frac{\\partial p_y}{\\partial z_y}\\\\\n",
    "& = { \\color{blue} -\\frac{1}{p_y} }  \\cdot { \\color{green} p_y(1 - p_i) } \\\\\n",
    "& = -(1 - p_i) \\\\\n",
    "& = \\underline{ { \\color{red} p_i - 1 } }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Case 2**: $i \\neq y$ (all others):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial z_i} & = { \\color{blue} -\\frac{1}{p_y} }  \\cdot \\frac{\\partial p_y}{\\partial z_i} \\\\\n",
    "& =  { \\color{blue} -\\frac{1}{p_y} }  \\cdot { \\color{green} (-p_y \\cdot p_i) }  \\\\\n",
    "& = \\underline{ { \\color{red} p_i } }\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Combining both cases:**\n",
    "We can write this compactly using one-hot encoding. If $\\mathbf{y}_{one-hot}$ is the one-hot vector of the true label, then:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{z}} = { \\color{red} \\mathbf{p} - \\mathbf{y}_{one-hot} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9a05c0-08fa-4733-b641-b933238d15da",
   "metadata": {},
   "source": [
    "##### Average loss (mini-batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dee14e-c8b3-4b3e-8091-416488d6eab6",
   "metadata": {},
   "source": [
    "When we have a batch of $N$ samples, we compute the **average loss** over the batch:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N} L_i$$\n",
    "\n",
    "To understand why we divide by $N$ in the gradient, consider the derivative of an average function. If $f(x_1, x_2, ..., x_N) = \\frac{1}{N} \\sum_{i=1}^{N} x_i$, then:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_i} = \\frac{1}{N}$$\n",
    "\n",
    "This is because when taking the partial derivative with respect to $x_i$, all other terms in the sum vanish (their derivatives are zero), we take the partial derivative with respect to the remaining term $\\frac{1}{N} x_i$ and only the constant $\\frac{1}{N}$ remains.\n",
    "\n",
    "Applying this to our loss: since $L = \\frac{1}{N} \\sum_{i=1}^{N} L_i$, we have:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{z}_i} = \\frac{1}{N} \\cdot \\frac{\\partial L_i}{\\partial \\mathbf{z}_i} = \\frac{1}{N}({ \\color{red} \\mathbf{p}_i - \\mathbf{y}_{i,one-hot} })$$\n",
    "\n",
    "For the entire batch, in vectorised form, this gives us:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{z}} = \\frac{1}{N}({ \\color{red} \\mathbf{p} - \\mathbf{y}_{one-hot} })$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9666a5-4924-4013-b5be-555dafd88a8b",
   "metadata": {},
   "source": [
    "### Manual Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a84687-2bb2-4696-bcf9-a59cbb8913f8",
   "metadata": {},
   "source": [
    "Caveat: this is not how this is usually implemented. The current function is very restrictive, and would break as soon as we changed the loss or activation functions, or if we had any other change in our model graph. The principled way is to build classes for all operations (addition, multiplication, activation functions, etc.), that store what their imputs are whenever they are used, and automatically compute the backward pass (gradient). Each operation only needs to know about its own inputs and the incoming gradient(s) from previous nodes in the graph. That allows for a full automation and the computation of gradients on arbitary graphs. See the [reference, \"Backpropagation\", the Andrej Karpathy & Sasha Rush courses](https://github.com/jchwenger/DLWP/tree/main/lectures/02/references.ipynb) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c445c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients_of_loss_wrt_weights(model, images_batch, labels_batch, predictions, loss):\n",
    "    \"\"\"\n",
    "    Computes gradients of loss w.r.t. weights using manual backpropagation.\n",
    "    \n",
    "    This function implements backpropagation from scratch without using GradientTape.\n",
    "    It works with NaiveSequential models containing NaiveDense layers.\n",
    "    \"\"\"\n",
    "    batch_size = keras.ops.shape(labels_batch)[0]\n",
    "    num_classes = keras.ops.shape(predictions)[1]\n",
    "    \n",
    "    gradients = []\n",
    "    \n",
    "    # Backpropagate through each layer in reverse order    \n",
    "    for layer_idx in range(len(model.layers) - 1, -1, -1):\n",
    "        layer = model.layers[layer_idx]\n",
    "        layer_input = layer._stored_input\n",
    "        pre_activation = layer._stored_pre_activation\n",
    "        \n",
    "        # Backpropagate through activation function\n",
    "        if layer_idx == len(model.layers) - 1:\n",
    "            # For softmax + sparse_categorical_crossentropy, the gradient w.r.t. pre-softmax logits\n",
    "            # simplifies to: (predictions - one_hot_labels) / batch_size\n",
    "            labels_one_hot = keras.ops.one_hot(labels_batch, num_classes)\n",
    "            current_grad = (predictions - labels_one_hot) / keras.ops.cast(batch_size, 'float32')\n",
    "        else:\n",
    "            # ReLU gradient: 1 if x > 0, else 0\n",
    "            relu_mask = keras.ops.cast(pre_activation > 0, 'float32')\n",
    "            current_grad = current_grad * relu_mask\n",
    " \n",
    "        # Compute gradients w.r.t. weights and bias\n",
    "        grad_W = keras.ops.matmul(keras.ops.transpose(layer_input), current_grad)\n",
    "        grad_b = keras.ops.sum(current_grad, axis=0)\n",
    "        \n",
    "        # Store gradients (in reverse order)\n",
    "        gradients.insert(0, grad_b)\n",
    "        gradients.insert(0, grad_W)\n",
    "        \n",
    "        # Compute gradient w.r.t. input for previous layer\n",
    "        # (in the forward pass, the output of the activation is now x, multiplied by w)\n",
    "        if layer_idx > 0:\n",
    "            current_grad = keras.ops.matmul(current_grad, keras.ops.transpose(layer.W))\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d90e3c-cc00-424e-87e4-0e3c6be227fa",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8fd0f27-c29e-412b-8a78-ed804d64f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    predictions = model(train_images[:5])\n",
    "    loss = keras.ops.sparse_categorical_crossentropy(train_labels[:5], predictions)\n",
    "    average_loss = keras.ops.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76892408-7403-4fe3-add7-2d7330757b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients_manual = get_gradients_of_loss_wrt_weights(model, train_images[:5], train_labels[:5], predictions, average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "088d1765-c273-42ef-83dc-12d8e59d36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients_tf = tape.gradient(average_loss, model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5adafb1a-fa1e-4f2e-befd-a6de2d9b45b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient 0: allclose=True, max_diff=4.19e-09\n",
      "Gradient 1: allclose=True, max_diff=4.19e-09\n",
      "Gradient 2: allclose=True, max_diff=1.49e-08\n",
      "Gradient 3: allclose=True, max_diff=1.49e-08\n"
     ]
    }
   ],
   "source": [
    "# Compare gradients with tolerance for floating-point precision\n",
    "for i, (grad_man, grad_tf) in enumerate(zip(gradients_manual, gradients_tf)):\n",
    "    is_close = np.allclose(grad_man.numpy(), grad_tf.numpy())\n",
    "    max_diff = keras.ops.max(keras.ops.abs(grad_man - grad_tf))\n",
    "    print(f\"Gradient {i}: allclose={is_close}, max_diff={max_diff:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e821d3-8a96-41c9-8973-77df450a696b",
   "metadata": {},
   "source": [
    "### Gradient computation in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877dcea-4245-4915-b4d8-ed50a5774afd",
   "metadata": {},
   "source": [
    "`tf.GradientTape` automatically computes the whole function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fd54016-fa0d-453c-859b-069de39b330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_training_step_keras(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        loss = keras.ops.sparse_categorical_crossentropy(labels_batch, predictions)\n",
    "        average_loss = keras.ops.mean(loss)\n",
    "    gradients = tape.gradient(average_loss, model.weights)\n",
    "    update_weights(gradients, model.weights)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dbea4d-d861-4eb4-ad22-5b3160884f96",
   "metadata": {},
   "source": [
    "## Running one training step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dfd4c0-fbd7-4a36-a7e9-7ef55b345579",
   "metadata": {},
   "source": [
    "### The weight update step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "582d2af4-9b16-42b6-a38a-8e3e51e9dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    for g, w in zip(gradients, weights):\n",
    "        # Assigns a new value to the variable, in place\n",
    "        w.assign(w - g * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10256ad3-5ce7-4d8e-b422-2d934cf7bfee",
   "metadata": {},
   "source": [
    "In practice, you'd use the implemented class (an object-oriented approach makes implementing running averages fairly neat, as the class object can hold these)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e13a73e2-2f6e-49b3-8df5-c05296cd1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "def update_weights_keras(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86e9f33a-e0dd-4138-9e1a-e58e2d67c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    # Runs the \"forward pass\"\n",
    "    predictions = model(images_batch)\n",
    "    loss = keras.ops.sparse_categorical_crossentropy(labels_batch, predictions)\n",
    "    average_loss = keras.ops.mean(loss)\n",
    "    # Computes the gradient of the loss with regard to the weights. The\n",
    "    # output, gradients, is a list where each entry corresponds to a\n",
    "    # weight from the model.weights list.\n",
    "    # Pass the per-sample loss, not the averaged one\n",
    "    gradients = get_gradients_of_loss_wrt_weights(model, images_batch, labels_batch, predictions, loss)\n",
    "    # Updates the weights using the gradients.\n",
    "    update_weights(gradients, model.weights)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea968bb1-396f-4938-846f-23a4ad4d8581",
   "metadata": {},
   "source": [
    "## The full training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d8a6dec-2b6b-402d-8426-cc421e58d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    # loop over the entire dataset for a number of epochs\n",
    "    for epoch_counter in range(epochs):\n",
    "        print(f\"Epoch {epoch_counter+1:>02}\")\n",
    "        # instantiate our batch generator\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "        # loop over all mini-batches in the dataset\n",
    "        for batch_counter in range(batch_generator.num_batches):\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            if batch_counter % 100 == 0:\n",
    "                print(f\"loss at batch {batch_counter:>03}: {loss:.2f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebc26450-694c-4906-a7b5-5ba2dc8bac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01\n",
      "loss at batch 000: 2.31\n",
      "loss at batch 100: 2.28\n",
      "loss at batch 200: 2.24\n",
      "loss at batch 300: 2.20\n",
      "loss at batch 400: 2.15\n",
      "\n",
      "Epoch 02\n",
      "loss at batch 000: 2.13\n",
      "loss at batch 100: 2.12\n",
      "loss at batch 200: 2.07\n",
      "loss at batch 300: 2.03\n",
      "loss at batch 400: 1.98\n",
      "\n",
      "Epoch 03\n",
      "loss at batch 000: 1.95\n",
      "loss at batch 100: 1.96\n",
      "loss at batch 200: 1.88\n",
      "loss at batch 300: 1.85\n",
      "loss at batch 400: 1.81\n",
      "\n",
      "Epoch 04\n",
      "loss at batch 000: 1.75\n",
      "loss at batch 100: 1.78\n",
      "loss at batch 200: 1.68\n",
      "loss at batch 300: 1.66\n",
      "loss at batch 400: 1.63\n",
      "\n",
      "Epoch 05\n",
      "loss at batch 000: 1.55\n",
      "loss at batch 100: 1.59\n",
      "loss at batch 200: 1.48\n",
      "loss at batch 300: 1.47\n",
      "loss at batch 400: 1.46\n",
      "\n",
      "Epoch 06\n",
      "loss at batch 000: 1.36\n",
      "loss at batch 100: 1.42\n",
      "loss at batch 200: 1.29\n",
      "loss at batch 300: 1.30\n",
      "loss at batch 400: 1.30\n",
      "\n",
      "Epoch 07\n",
      "loss at batch 000: 1.20\n",
      "loss at batch 100: 1.26\n",
      "loss at batch 200: 1.13\n",
      "loss at batch 300: 1.15\n",
      "loss at batch 400: 1.17\n",
      "\n",
      "Epoch 08\n",
      "loss at batch 000: 1.06\n",
      "loss at batch 100: 1.12\n",
      "loss at batch 200: 0.99\n",
      "loss at batch 300: 1.02\n",
      "loss at batch 400: 1.06\n",
      "\n",
      "Epoch 09\n",
      "loss at batch 000: 0.95\n",
      "loss at batch 100: 1.01\n",
      "loss at batch 200: 0.89\n",
      "loss at batch 300: 0.92\n",
      "loss at batch 400: 0.98\n",
      "\n",
      "Epoch 10\n",
      "loss at batch 000: 0.87\n",
      "loss at batch 100: 0.92\n",
      "loss at batch 200: 0.80\n",
      "loss at batch 300: 0.84\n",
      "loss at batch 400: 0.91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878ace9-7636-498a-8d9c-73889f813a71",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3b69f07-edca-49e4-b01f-a5b5bf9baa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy: 0.84'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(test_images)\n",
    "predicted_labels = keras.ops.argmax(predictions, axis=1)\n",
    "matches = predicted_labels == test_labels\n",
    "f\"accuracy: {keras.ops.mean(matches):.2f}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
