{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revision guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exam will be held in person on campus. **Duration: 2h 15 minutes**.\n",
    "\n",
    "- *Part A*:\n",
    "  - Fundamentals I, 40 marks (DLWP Chapters 1-6), 45 minutes\n",
    "- *Part B*, **choose two questions** (out of these three):\n",
    "  - Fundamentals II (DLWP Chapters 1-6), 30 marks, 45 minutes\n",
    "  - Computer Vision (DLWP Chapters 8-12), 30 marks, 45 minutes\n",
    "  - Text & Sequences (DLWP Chapter 13-15), 30 marks, 45 minutes  \n",
    "\n",
    "The questions will test concepts and skills. The following lists are not necessarily exhaustive but they should give a good indication of the examinable parts of the syllabus. \n",
    "\n",
    "Warning: use only the resources from the module (my lecture notebooks and DLWP).\n",
    "\n",
    "There are four marks per question and each question carries four responses. You should choose the best response. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics 1-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1:    `chapter01_what-is-deep-learning.ipynb`  \n",
    "2.1:  `chapter02_mathematical-building-blocks.1.first-look.ipynb`  \n",
    "2.2:  `chapter02_mathematical-building-blocks.2.data-representations.ipynb`  \n",
    "2.3:  `chapter02_mathematical-building-blocks.3.optimization.ipynb`  \n",
    "1+3:  `recap.ipynb`  \n",
    "4.1:  `chapter04_classification-and-regression.1.imdb.ipynb`  \n",
    "4.2:  `chapter04_classification-and-regression.2.reuters.ipynb`  \n",
    "4.3:  `chapter04_classification-and-regression.3.california.ipynb`  \n",
    "5:    `chapter05_fundamentals-of-ml.ipynb`  \n",
    "6:    `chapter06_universal-workflow.ipynb`\n",
    "\n",
    "| Concept | Lecture notebook |\n",
    "|:---|:---|\n",
    "learning | 1\n",
    "statistical model | 1\n",
    "layer | 1, 3\n",
    "depth | 1\n",
    "deep | 1\n",
    "neural network | 1\n",
    "vectorisation | 1\n",
    "weights, biases | 1, 2.1, 2.3\n",
    "samples | 1, 2.3\n",
    "loss function | 1, 2.3\n",
    "optimiser | 2.3\n",
    "backpropagation | 2.3\n",
    "one-hot | 4.1, 4.2\n",
    "softmax | 2.1, 4.2, 1+3\n",
    "probability distribution |  2.1, 4.2, 1+3\n",
    "categorical crossentropy | 4.2, 1+3\n",
    "accuracy | 2\n",
    "epoch | 2.1\n",
    "dense layer | 2.1\n",
    "fully connected | 2.1\n",
    "tensor | 2.2\n",
    "dimensionality | 2.2\n",
    "length (of a vector, 1D tensor) | 2.2\n",
    "axis | 2.2\n",
    "rank | 2.2\n",
    "shape | 2.2\n",
    "slicing | 2.2\n",
    "mini-batch | 2.2, 2.3\n",
    "activation | 2, 1+3\n",
    "element-wise | 2.2\n",
    "broadcasting | 2.2\n",
    "tensor dot product | 2.2\n",
    "reshaping | 2.2\n",
    "transposition | 2.2\n",
    "linearity  | 2.3\n",
    "forward pass | 2.3\n",
    "gradient descent | 2.3\n",
    "derivative | 2.3\n",
    "stochastic gradient descent | 2.3\n",
    "learning rate | 2.3\n",
    "momentum | 2.3\n",
    "backpropagation | 2.3\n",
    "backwards pass | 2.3\n",
    "affine transformation | 2.2, 2.3, 1+3\n",
    "targets | 2.3\n",
    "sigmoid, relu, tanh activations | 1+3\n",
    "layer transformation | 1, 2.2, 2.3\n",
    "hypothesis space | 4.1, 5+6\n",
    "unit | 4.1\n",
    "binary cross-entropy | 4.1\n",
    "training, validation and test sets | 4.1, 4.3, 5+6\n",
    "underfitting/overfitting | 4.1, 4.2, 4.3 5+6\n",
    "information bottleneck | 4.2\n",
    "regression | 4.3\n",
    "normalisation | 4.3\n",
    "hyperparameter | 4.1, 4.2, 4.3, 5+6\n",
    "generalisation | 5+6\n",
    "information leak | 4.1, 5+6\n",
    "simple hold-out validation | 5+6\n",
    "K-fold validation | 4.3\n",
    "iterated K-fold validation | 5+6\n",
    "feature engineering | 5+6\n",
    "regularisation | 5+6\n",
    "capacity | 5+6\n",
    "regular (parameters) | 5+6\n",
    "weight regularisation | 5+6\n",
    "dropout | 5+6\n",
    "last-layer activation and loss for problem type | 5+6\n",
    "statistical power | 5+6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 DL for computer vision and 6 DL for texts and sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 8:    `chapter08_image-classification.ipynb`  \n",
    "- 10:    `chapter10_interpreting-what-convnets-learn.ipynb`\n",
    "- 13:   `chapter13_timeseries-forecasting.ipynb`  \n",
    "- 14.1: `chapter14_text-classification.1.nlp-ngrams-embeddings.ipynb`  \n",
    "- 14.2: `chapter14_text-classification.2.sequence-models.ipynb`  \n",
    "- 15: `chapter15_language-models-and-the-transformer.ipynb`\n",
    "\n",
    "| Concept | Lecture notebook |\n",
    "|:---|:---|\n",
    "convolutional layer | 8\n",
    "filters | 8\n",
    "local receptive field | 8\n",
    "pooling | 8\n",
    "data augmentation | 8\n",
    "(pre-trained) convolutional base | 8\n",
    "freezing | 8\n",
    "fine-tuning | 8\n",
    "response of lower and higher layer filters | 10\n",
    "heatmap | 10\n",
    "recurrent layer transformation function | 13 \n",
    "SimpleRNN layer | 13 \n",
    "RNN pseudocode | 13 \n",
    "LSTM | 13 \n",
    "gates | 13 \n",
    "GRU | 13 \n",
    "bidirectional RNN layer | 13 \n",
    "1D convnets | 13 \n",
    "embedding layer | 14.1 \n",
    "word embeddings | 14.2 \n",
    "transformer | 15 \n",
    "attention (query/keys/values) | 15 \n",
    "self-attention | 15 \n",
    "cross-attention | 15 \n",
    "(causal) masking | 15 \n",
    "positional encoding | 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics 1-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify rank and shape from Numpy code\n",
    "2. Code to extract parts of a tensor \n",
    "3. Identify number of samples in an input tensor of given shape\n",
    "4. Python slicing\n",
    "5. Extracting the n'th mini-batch\n",
    "6. Activation of a vector from the graphical form of ```relu, sigmoid``` and ```tanh```\n",
    "7. Calculation of $\\text{relu}(w\\cdot x + b)$ from $w, x, b$.\n",
    "8. One-hot encode a very short text using a short dictionary\n",
    "9. Recognise weight and bias shapes in a specified layer\n",
    "13. Infer the intended problem from the model\n",
    "11. Infer the number of elements of the input tensor from model code\n",
    "12. Calculate number of trainable parameters in a weight/bias tensor for a given layer and total number of trainable layer parameters\n",
    "13. Guessing a possible ```softmax``` output\n",
    "14. Use bottleneck avoidance to guess numebr of units in a layer\n",
    "15. Missing arguments in compilation statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 5: Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Output shape of a convolutonal layer\n",
    "2. Number of trainable parameters/filters in a convolutional layer\n",
    "3. Output tensor shape of pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 6: Text and Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Embedding layer output shape and number of trainable parameters\n",
    "2. The number of trainable parameters in a simple RNN layer\n",
    "3. The time complexity of Transformers, and compared with RNNs/ConvNets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
